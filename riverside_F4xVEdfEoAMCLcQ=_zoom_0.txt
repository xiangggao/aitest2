:
So quick second. It's great to meet you. Sorry I was a couple minutes late by the way. That's okay. Hi. Good to meet you too. Great. Cool. I think we can get started then. Let's do it. All right. Great. So I've more or less met everybody here. My name is Chris McQuiston and I, there's actually a lot to talk about on each of these things. So I don't have any expectation of getting through all of the projects on this deck. So please feel free to jump in with any questions that you have, or if you wanna jump to another project that you've seen on my website, feel free to jump in. The first one I'm gonna talk about though is the Amazon seller and developer provider experiences. This was for the seller partner services team. If you're not aware of what a seller partner is at Amazon, Seller partners are the third party sellers that sell the majority of product on Amazon. Most of the things that you see for sale on Amazon are not directly from Amazon, but are sold by these third party providers. So there is a seller partner experience on the Seller Central website for these customers, or seller customers, I should say. And the... There has historically been a challenge in serving these customers. And that is the fact that this website that we have does not have all the features that they need in order to run their business. And in order to fill that gap, third-party developers have come in and created applications for these people to help them sort of fill the gaps that Solar Central hasn't filled. And... This has created some security challenges for Amazon and it's also created some trust issues for the customers, the seller customers, because they don't necessarily know if the developer that they're in contact with to provide these services are trustworthy. They don't know about sharing their customer data to people and stuff like that. So the seller partner app store. was a net new concept that we had a new app store for that seller partner space that was going to solve on the seller side, allowed the sellers to discover these third party apps, allow them to compare and review apps, and have some confidence that these applications are going to be vetted by Amazon and are more trustworthy. It gives them a streamlined way to manage their permissions and the permissions that they're sharing with these developers and gives them also streamlined way to register these developers with their accounts so that they can share specific types of data. In the past, this was done through a set of services that were not as secure and basically gave the developer full access to all of the data on the customer sites. Some of the changes that were introduced was this idea that when a developer registers an application, they're very specific about the data they need to access. And that's the only when they, when a customer gives permission That's the only data that that developer will be able to access. And, um, so this helped with the, with filtering out some of the bad actors. This helped with, uh, giving the customers that trust that their data is secure and safe. And as the sole designer on this, I was, uh, working with closely with the engineering and product team. To. create this entire experience on both sides, making sure that they all connected together and made sense to everyone. This also included streamlined OAuth registration so that if you had already, for example, a relationship with BigCommerce, you could actually launch and authenticate the connection between BigCommerce and Amazon from your BigCommerce space rather than needing to launch it from the Amazon site. So the outcomes of this were pretty good. This is why this is the first one for me, because the amount of impact that we had, Amazon saw a marked increase in pro sellers worldwide that were using the services. And these pro sellers who generate significant sales in Amazon, this uptick in app and service usage correlated with an 8.2% bump in seller gross merchandise volume. So that was pretty significant for us. This was also the highest rated experience in seller central history. We have a sort of a how's my driving score where the seller users can give feedback on specific interfaces and stuff like that. And it's maintained that top score. Well, up until the time I left, I haven't tracked it since. And because of the way that we approached design and having sort of this North Star vision of what this is gonna be when it grew up, after launch, we were immediately able to very quickly respond to any negative feedback that we had. We responded to the top five customer feedback themes, representing 80% of the negative feedback. within the first few months after launch without incurring any technical debt. What were these customer themes? They were issues, a lot of them were things around whether or not they were gonna get certain features. So we launched without search, for example. But because we already had a plan for search and we knew how that was gonna work and we were in the process of testing it when we were getting this feedback, that was a quick response. But there were other things like... We had plans for certain features like how we were going to represent filtering and that sidebar experience for, sorry. this Amazon-like sidebar experience and the types of categories and stuff, how they were mapped and how you could explore that. And we had a couple of different ways that we were going to explore doing this. We tested it and we went with one way in development. And then when we got the feedback from the live product and people are actually able to click through it and get the experience. we got the feedback that they would have preferred it the other way, one of the other ways that we had it. And because we had that design all queued up, we were able to go ahead and jump in and make that happen quickly. We were able to quickly move on things like the type of information that was available at the tile level so that people could quickly scan the same way they do on the retail site. and just in terms of understanding the priority of the information that's being served there, so they can help make decisions to dig deeper or not. So sorts of things, things that we had already planned or things that we knew there were multiple options for, that sort of thing. And because we took a rather modular approach, we had the flexibility to pretty quickly switch up any workflow issues that we had, for example, during the registration or anything. If there's a friction there, we're able to change the order of some of those steps, and so on. Any other questions about this? This is probably the fastest I've flown through this so far. Awesome. Yeah, no, we can move on. Okay. Eric, go ahead. Sorry, one quick question. Were there... I know the growth, the impact was significant. I'm just interested to know, were there any surprises in terms of non-performant components of the new design that you... I know there was customer feedback, but anything where... when you rolled it out and you were analyzing performance, you said, oh, that's interesting like that, isn't working as well as we thought. Actually, no, there weren't any surprises like that because there were a lot of compromises we made for the first iteration. So most of the stuff that we came across that people were complaining about, we kind of knew they were gonna complain about. Like some of the features that we had planned for later sprints and so on. We knew it was going to be a friction point if they didn't have it at launch and that we would be able to address as we went on. And then one other quick question. The merchandise volume went up. I'm curious to know what were some of the, what were some of the metrics, that's obviously like the main KPI, like it created Lyft, you know, for that component of the business. What were some of the other metrics that sort of laddered up to that, that you used to measure the success of it? Well, the other things that we, we had certain targets in terms of the number of people that would use it. and how much click through we would get to the target sites and how much conversion we would get to an actual sale. And so all of those were very positive but I can't give specific numbers on those. Yeah, yeah, no, that makes sense. Just, yeah, that makes sense. The basically conversion rates in workflow. Yeah, yeah. And yeah, and because the first iteration didn't handle the full sales, like actually paying through Amazon. It was actually a challenge to track that conversion funnel. But once that portion where customers were paying through Amazon, rather than just being redirected to the website, once that was happening, then there was a lot more that we could track. Yeah. Cool. Makes sense. All right. So a little bit about some of the, for each of these projects, I try to call out some things that were maybe a little exceptional about the process and getting them there. And one of the things that was great about working on the Spectrum product was how, Spectrum was the name of the app store, the entire app store piece and all the services that went beyond that. That was the program name. But one of the things that was great about it was the collaborative work between product design and development. The product team was open to this to a level that I'd rarely seen before in other companies. So one of the ways this was beneficial is that we started this, we had a customer journey mapping exercise at the beginning of every fiscal year in order to help drive operational planning. And we started this when I first got there, when we didn't have an actual user flow for the application yet, for the app store yet. But we continued to do it in the following years so that we... pretty much we're all in alignment in terms of where we wanted the product to be as we're doing our operational planning and what we're all agreeing to the scope of what we're all agreeing to build out. And so this was primarily a card sorting and grouping operation and some choice SMEs and some of the executive leadership team to kind of be involved in this process. And we would prime the pump as it were with an empathy mapping exercise that would help to identify the roles. We weren't calling them personas, we're calling them roles, but the roles and have an emotional understanding of their personal goals. This was actually proved, this aspect of it proved itself particularly valuable in the first time that we did this because the product team very quickly realized that they weren't serving the global seller the way they needed to be. And it led to a rethinking of how we needed to coordinate the customer or the seller accounts and share the data across the different regions. Throughout the year, if we got some particularly tricky technical problems that we needed to solve or interactions we needed to solve, quite often the biggest disconnect that we would have is not necessarily between design and product or design and engineering, but it's more about the disconnect between engineering and product. Because there would be these business rules that would be kind of outlined in the operational planning phase. And the tech team would have some very specific ideas of how that would be done, but it's not necessarily in alignment with what the experience needed to be from the business perspective. So I would, when some of these would come up, I would pull the teams together and we would run through some specific exercises to sort of design that flow so that everybody was understanding what the technical problems were and the best way to address them. The exercise that worked the best for this was an exercise where we would have two separate teams and they would both come up with their own design and I would facilitate this. They would come up with their own design for the flow, talk through it, and they were encouraged to draw as little or as much as was necessary to communicate the flow. And then at the end, everybody would review it and we would be able to kind of pick the best bits that work from each of the designs and come to an agreement as to what our best possible flow would be there to solve the business problems. I'm kind of waving my hands as I'm talking, but please, if you've got any questions about that. Another aspect that was important about this engagement was that this seller app store had to be built within the context of a larger application ecosystem on the seller partner platform. And on the one hand, there was some visual design challenges in terms of trying to find this balance between the seller partner at store experience and the retail experience that we want to bring in. Because those users are familiar with that, but the biggest challenge that we had from a UX side was I kept on running into UX debt as I was evaluating the rest of the environment and. some of this UX debt was impacting what I could actually accomplish in this Solar Partner App Store. And the biggest problem with it is that the organization as a whole was not tracking UX debt. Typically, if a UX issue was reported, it was captured as a ticket. And at the end of the sprint or during the sprint, there would be a prioritization exercise and... they would either address the ticket or not address the ticket. If they didn't address the ticket, it was typically closed as we're not gonna fix this. And so that bit of UX debt was just lost to history. And if you didn't have this tribal knowledge of all these elements that had been found and were ongoing issues, you were discovering all this stuff anew. So what... I did to address this was come up with a UX tracking mechanism. And I had to use the existing ticket system. But I tied it to a wiki where I basically created an heuristic checklist. And the reason why I went with the heuristic checklist is one of the reasons, one of the challenges that designers had when they came across an issue was convincing the product and engineering teams of the value of making the change. The teams were very quantitative focused in terms of, hey, bring us the data that shows that this would actually move the needle on the experience. And there weren't really very favorable towards heuristic explanations, especially when some of the junior designers weren't able to really describe the situation in a way that was convincing. What I did was I created this heuristic checklist that allowed the designers to attack that issue in two different ways. One, either they came across an issue, they could use the checklist to figure out what's the closest heuristic that is being violated here. And then when they find that, they can create a SIM. And the SIM is pre-populated with an explanation of the heuristic, what the impact of violating the heuristic, the justification for... fixing the heuristic and the, well, at least the first step at what the relative priority of this type of bug should be. And the other way that this, that they could use this was when they're going through an evaluation of their own designs, they can go through the heuristic checklist to see if their design is violating any of these issues. They also were using it for evaluation of the existing evaluation of the existing experience as well. So in order to get adoption of this, I was shopping this around with the various design teams internal to the Solar Partner Services team and getting them on board with the idea of creating the centralized UX database that everybody could keep coming back to, that the designers could use to sell these issues they knew needed to be fixed, but also going, I also went to the product teams and showed how, hey, when they're getting ready for their operational planning, and we know that there's a UX debt that you should probably address, here's a tool for you to go in and do a search on your area and find these issues that have already been found, and to some extent, prioritized for you, so that you can see what kind of stuff would move the needle for your product. Once I had completed the system and I got the first few teams on board, leadership stepped in and said, hey, this is a great system. We're going to mandate that everybody use the system to evaluate and address X percent of UX issues that they find in their queue for the next operational planning cycle. And as far as I know, they're still using this. but that was the mechanism that I created to kind of deal with this UX debt issue. I'm sure you guys don't have the same kind of UX debt issues. You're probably all on top of tracking these things, but this type of process and mechanism thing is some of the stuff that I think about when I'm trying to work with teams and how we engage and make sure that we're crossing all our T's and... dotting all the i's. Any questions about this? I know there's a lot and it's pretty dense, and if you need me to move on faster let me know too, but is there any questions about this so far? Nope. Let's move on. I guess a quick question. How many sections do you have? Just want to get a good sense. I've got a lot of sections. I got more than we'll be able to get through. So I mean, I can talk about Avalara, which is the biggest story about Avalara is the onboarding flow. I can talk about the Microsoft Global Marketing Organization and that story is a lot about BI and design language management. The Kindle app developer experience is about really about pushing an IA thing to support a platform. And those are the projects. So if there's a topic that you would like to cover to make sure that anything that you need to know about me and how I work, let me know and I can jump to the project that's most relevant. Awesome. I think like some, some of the work that might be doing this quarter is around kind of like onboarding and activation. Love to go through kind of this set of slides that you have around Avalara. Sure. So with Avalara, I was actually brought in because they were reinventing their back and, and they needed to, basically they had a scaling problem with their business. They had already saturated all of the large businesses and they recognized that in order to grow as a company, they needed to be able to address the small and medium business, as well as the home office business owners. The challenge there was that their current onboarding system was very high touch. It was very much a sales-oriented organization. So the setup process for getting somebody set up with the system was around two weeks on average. a lot of calls back and forth and so on. So the only way that we could scale properly would be to create a zero touch experience. And in order to accomplish this, I had to really dive deep with all of the SMEs that we had, did a lot of testing with customers along the way, but was primarily about coming to understand how taxes were calculated. from region to region, district to district, state to state, and so on. What I did was I worked with the SMEs to identify all the decision points and inflection points that told us whether or not we needed more information for the system table, be able to properly calculate sales tax, and at the end we had an onboarding flow that took that two-week time frame down to about 15 to 30 minutes. And the flow was along the lines of, you know, they enter in their basic information. So we know where their business is located. We asked them where they're responsible for for calculating taxes for. So where do these other places that they're doing business in? And if any of those places are first at the state level, we trigger. We've got certain states that we know we need more information on. So if they're that they have California or Alabama. We go in and we ask specific questions about the details of how and where they do businesses and specific districts for those states. But even with that piece, that drill down usually didn't take more than 30 minutes. So that was a big plus and that's how that worked. I mean, you can't see the detail here, but this is just like the various flows and branches for the paper prototypes that we used. We did, as I say, test this with customers along the way pretty heavily. just because it was very important for us to make sure that we got this right, that it was intuitive and people understood what was going on because the idea was that if we're hitting these smaller businesses, a lot of the people have no idea even what the responsibility is towards paying taxes in certain places and so on. So we just had to be able to educate them along the way. So there was a lot of focus on that. Great. I also kind of deconstructed the entire interface so that, you know, after understanding and interviewing these customers, understanding what their priorities are and why they would come back to this experience, I learned that the only reason why they would come back is if there is a dispute between them and a customer about how much the customer is charged. So their priority was to be able to come back, to be able to drill into specific transactions or get reports so that they could file their taxes. And so a lot of the stuff that was on the surface was really about setting the context for the calculations. And so that was all stripped out and put into the settings area. And we were able to just focus on these three areas. And returns is essentially another service that was available to them, but they didn't necessarily have to buy. This was another one where workshops were very helpful, but even more so we didn't have a big QA budget. We didn't have a dedicated QA team or dedicated team of testers for the UX itself. So I teamed up with an engineering team to set up regular bug bashes where we would pull as many people in from the company as possible. to sit down for four to six hours and there would be food and beer and prizes for bugs and things like that. And so it was kind of a fun exercise to get everybody involved in finding any issues with a focus on the UX issues. Any questions about the Avalara stuff? Because you were saying that there was a interest in the onboarding. One question that I mean it's not specific to the Avalara case but a little bit of a more general one at least for the two cases that you have presented so far. What was the size of the teams involved from for design and how long the projects took. Okay, so the We have a core UX team that's for the entire platform. I was the only designer on the app store. Okay. That was just me. And it was two teams of developers. So there's a couple of dozen developers and a team of, depending on what time of year we're talking about, because we had a little bit of turnover between three and five product managers. And this was an ongoing project. We took it to launch in the first year. And then year two was about backfilling a lot of the features that we knew that we wanted from year one that were already designed, but queuing them up to get built in year two. So basically I had designed in year one enough work for those teams that it took them about two and a half years to build out. most of those features. Then also year two was a lot about responding to any negative feedback that we had but those were relatively quick and fast and some of them are already part of the roadmap as I mentioned. So the main bottleneck that you saw in this case was from product because of the retention there and like product models coming in? Oh, it was actually engineering capacity was the biggest bottleneck. Okay. That was the biggest thing. The same is true of the Avalara case. That the Avalara team, I had a small team of two designers that I was leading and I had to kind of rehabilitate the relationship between those designers and the rest of the company because it was pretty toxic when I came in. But once we were all moving right along, it was actually very smooth. That was a little bit, I mean, engineering was still the bottleneck, but it was less of a bottleneck because the first couple of months, we were playing catch-up on the engineering side. trying to catch up with the stuff that the engineering was trying to do. So when I first got there, design was the bottleneck. And then it became the engineering team that was the bottleneck. They did not have a product team when I got there. And so I had to take on the role of the product owner as well as setting the large arc of the design and tightening up the design language. and setting out the major flows. The other designers who were more junior did a lot of the detail work in terms of digging in, making sure that all the edge cases are covered, making sure that we're handling all the alert cases and doing red lines and things like that. So that was kind of a unique space. I had to act, as I was saying, as the product owner to make the decisions of what's gonna make this the best product, but I was also spending that time educating the executive team about why they needed a product manager. And they did eventually start building out a product management team. At the time that I left, they had two product managers and a manager over them. Okay, okay, thank you. No more questions on this one. Okay, so the Microsoft Global Marketing Organization. I came in on this project as a vendor. So I was the sole designer. There was a large team, I'm not sure how large, of vendor engineers in India. So that was an interesting logistic issue in terms of working with a remote team in a diametrically opposed time zone. This was another case where when I came in, there wasn't a clear understanding of what a product manager needed to do. I had to educate the analyst that was acting as a product manager about why business goals are important and understanding what the return on investment needed to be in terms of what defines success for this project. Beyond that, the overview of the project itself was the global marketing organization was reinventing its reporting and business intelligence systems. They had hundreds of reports that they had for their teams across multiple verticals, customer insights team and so on. There was like six teams. I don't remember with all of them were. I think that included mobile and a couple of others. So they were reinventing their IA and how they wanted to do it. They wanted to strip out all the extraneous reports and bring it down to something that was manageable and making sure that everybody's looking at the same data. So I worked with them over two years. to develop a new BI system where they would basically solve for everybody at every level had a dashboard that they could start with and the ability to drill down to the granular data level so that they could first of all come into the work in their dashboard. see any indications of there's anything that they need to be alert about and be able to drill down into those areas to see why things are performing certain ways. This was the mandate here was to try to make this work like a Windows modern interface, which was kind of an interesting challenge because Windows modern was not designed to handle this sort of data density. So instead what I did was I took the, broke it down to first principles for the Windows Modern, which was essentially to create signposts. And I took that design approach to kind of do what I was saying was that you would have a dashboard, you would be directed by the system by saying, okay, we know at this level, here's the questions that you want to answer. Instead of giving them a bunch of tabular data, we actually answered those questions for those users at the top of the page. if they needed to drill in to find out why those answers were the way they were, they had the ability to drill down until they got to that granular data and they could do the analysis themselves. But the point was to reduce the amount of friction people had to go through the amount of cognitive overhead they had to deal with in that where they were kind of trying to analyze just these huge grids of data. And So in order to get there, I was working with SMEs and all of the verticals. And in terms of understanding the specific views that all of these different roles needed to do in order to do their job and design them accordingly using the design language that I developed. Any, so yeah, stated here over the Custiers site. evolved. I worked closely with the teams in order to build this in such a way that the roadmap was not disruptive, that people weren't just shut out of the work experience they're used to. They had the ability to transition and make a soft landing into the new experience. And at the end of it, The executive team had the ability to come in at the top of the site and be able to drill into any vertical and dig as deep as they want to understand how their business was working. That was the goal and that's what we achieved. A lot of stuff that in order to make this work and in order to make this scalable, because there was an end date, I wasn't gonna work with them forever to make this happen. And they wanted to get more and more of their internal teams on this platform. I needed to... not just build the design language that was extensible, I needed to create a design library, a component library, make sure that the developers had a component library that was tied to the design library, guidance on how these pages are assembled and the thinking behind the layouts and how that's supposed to work. Sorry to correct you up, Chris. We had the... The Zoom room is actually going to be cutting off again in about a minute. So we have another 15 minutes left. Do you mind coming back to this original room? Just gonna share it in the chat now. Sure. Okay, sorry about that. It says I'm apparently not a part of the Zoom org. We'll get that fixed. No problem. Right after this. Alright, see you.